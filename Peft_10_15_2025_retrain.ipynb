{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6280d99-4322-43be-b10a-6761e2e97a51",
   "metadata": {},
   "source": [
    "#### Student: Rodrigo Quezada Reyes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad2f77-ef7c-496f-865e-3491222b02ed",
   "metadata": {},
   "source": [
    "#### The Udacity platform was having Connection Failed over and over again during training or evaluation so rather perform locally with my GPU and will submit as a zip file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e5050f-c7f5-4896-8c5f-65b20e657af0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "Describing my choices for each of the following:\n",
    "\n",
    "* PEFT technique: Lora as I find it a great option for fine-tuning while freezing a lot of paremeters for computation effiency.\n",
    "* Model: deberta-v3-small because it is a great model for text classification tasks and this is one of those.\n",
    "* Evaluation approach: Performing initial evaluation with the foundational model, then performing the same evaluation using the trained Peft Model. This will allow a fair comparison of the model as is compared to the model fine-tuned.\n",
    "* Fine-tuning dataset: Hugging Face tweet_eval dataset as it is an interesting collection of tweet-based benchmark tasks designed for evaluating text classification models on social media content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "* I am selecting to load as my chosen pre-trained foundational model, the deberta-v3-small after assessing its capability for classification tasks.\n",
    "* I will evaluate its performance prior to fine-tuning and then after fine-tuning. \n",
    "* I will also include loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d21f21",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e94f0",
   "metadata": {},
   "source": [
    "#### Import all dependencies including the Hugging Face PEFT Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335bc74f-d9a3-45c6-9fe3-f81c7f80a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ccc73b0-8dc6-4733-9ab6-e319da8ea71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b96d81-b39e-4b37-a763-44a19db347b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2feb3a30-517b-4ca9-ae67-ba567608d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f7ed12f-b484-44d2-8205-aa6bd78b8c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu126\n",
      "12.6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Importing the torch library\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "889ac152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the rest of the libraries\n",
    "\n",
    "from peft import LoraConfig\n",
    "from peft import TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from peft import get_peft_model\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig, AutoPeftModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "from transformers import pipeline, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from transformers import DebertaV2ForSequenceClassification, DebertaV2Tokenizer\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "import sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d87fa2ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "# Verify if GPU is available\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c536757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting this up because my session expires as it times out while I am training the model at Udacity platform\n",
    "# now running locally so no longer needed\n",
    "\n",
    "#import time\n",
    "#import threading\n",
    "\n",
    "#def keep_alive():\n",
    " #   while True:\n",
    "  #      print(\"Keeping the session alive...\")\n",
    "   #     time.sleep(300)  # Sleep for 5 minutes\n",
    "\n",
    "# Start the keep-alive thread\n",
    "#keep_alive_thread = threading.Thread(target=keep_alive)\n",
    "#keep_alive_thread.daemon = True  # This allows the thread to exit when the main program does\n",
    "#keep_alive_thread.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c7fde-21de-484e-86a7-61fea432dbbd",
   "metadata": {},
   "source": [
    "### Function to perform evaluate model performance (same function will be used for before and after model fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54d2b4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e73deae22f4ce5bfe0069f224ae7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to evaluate both the initial performance of the pretrained model with the dataset \n",
    "# to then to use to evaluate the pretrained fine-tuned model with the dataset \n",
    "\n",
    "kpi = evaluate.load(\"accuracy\")\n",
    "\n",
    "def model_evaluating(model, dataset, batch_size=1):\n",
    "    model.eval()\n",
    "    model.to(\"cuda\")\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    for i in dataloader:\n",
    "        input_ids = i[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = i[\"attention_mask\"].to(\"cuda\")\n",
    "        labels = i[\"label\"].to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "        \n",
    "        kpi.add_batch(predictions=predictions, references=labels)\n",
    "    \n",
    "    return kpi.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863e0b5e",
   "metadata": {},
   "source": [
    "## Loading the selected pre-trained model and Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "Creating a PEFT model from my loaded model, run a training loop, and saving the PEFT model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a12c5e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3cd8b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Creating a model object from a Converting a Transformer Model by loading my chosen pre-trained Hugging Face model\\n\\nmy_model = DebertaV2ForSequenceClassification.from_pretrained(\\n    'microsoft/deberta-v3-small', \\n    num_labels=4,\\n    ignore_mismatched_sizes=True,\\n    use_safetensors=True \\n    )\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Creating a model object from a Converting a Transformer Model by loading my chosen pre-trained Hugging Face model\n",
    "           \n",
    "my_model = DebertaV2ForSequenceClassification.from_pretrained(\n",
    "    'microsoft/deberta-v3-small', \n",
    "    num_labels=4,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    use_safetensors=True \n",
    "    )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c9ebc-2241-4dab-99ec-ba3c6b945d10",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "Because of what I have gone through on mismatches of the number of labels at the last layer. \n",
    "I am now configuring, saving, and loading the base model to ensure the adapter and head are aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81291e69-ee42-4a9b-a920-3d23b99ddd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6524d56f8d194de1bce7a51a81f91a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dslab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dslab\\.cache\\huggingface\\hub\\models--microsoft--deberta-v3-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef8fd919384445787094e94a8fc9737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72dc42f69774d999a709fb9b3c42b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model at: ./saved_models/initial_lora_model_8282025_v01\n"
     ]
    }
   ],
   "source": [
    "# Creating a model object from a Converting a Transformer Model by loading my chosen pre-trained Hugging Face model\n",
    "\n",
    "# Original model load\n",
    "#my_model = DebertaV2ForSequenceClassification.from_pretrained(\n",
    " #   'microsoft/deberta-v3-small', \n",
    " #   num_labels=4,\n",
    " #   ignore_mismatched_sizes=True,\n",
    " #   use_safetensors=True \n",
    " #   )\n",
    "\n",
    "initial_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-small\",\n",
    "    num_labels=4   \n",
    ")\n",
    "\n",
    "#1) Create a PEFT model from your loaded mode\n",
    "\n",
    "# Creating a PEFT configuration object from a Lora function\n",
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "# Creating a trainable PEFT model object from a PEFT function\n",
    "initial_peft_model = get_peft_model(initial_model, peft_config)\n",
    "\n",
    "version = \"v01\"  \n",
    "save_dir = f\"./saved_models/initial_lora_model_8282025_{version}\"\n",
    "\n",
    "#final_model = original_lora_peft.merge_and_unload()\n",
    "initial_peft_model.save_pretrained(save_dir)\n",
    "\n",
    "print(\"Saved model at:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84e89a9e-1f26-45a7-9098-d197e007a06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels: 4\n",
      "Adapters loaded: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dslab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\safetensors\\torch.py:315: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  result[k] = f.get_tensor(k)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Loading the save LoRA model:\n",
    "\n",
    "# Loading the base model with the correct number of labels\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-small\",\n",
    "    num_labels=4\n",
    ")\n",
    "\n",
    "# Step 2: load the PEFT adapter on top of it\n",
    "lora_peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./saved_models/initial_lora_model_8282025_v01\"  \n",
    ")\n",
    "\n",
    "# Step 3: Verification step\n",
    "print(\"Num labels:\", lora_peft_model.config.num_labels)\n",
    "print(\"Adapters loaded:\", hasattr(lora_peft_model, \"peft_config\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 1) Create a PEFT model from your loaded mode\\n\\n# Creating a PEFT configuration object from a Lora function\\n\\nmy_config = LoraConfig(\\n    task_type=TaskType.SEQ_CLS,  # For sequence classification\\n    r=8,\\n    lora_alpha=32,\\n    lora_dropout=0.1\\n)\\n\\n# Creating a trainable PEFT model object from a PEFT function\\n\\nlora_peft_model = get_peft_model(my_model, my_config)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 1) Create a PEFT model from your loaded mode\n",
    "\n",
    "# Creating a PEFT configuration object from a Lora function\n",
    "\n",
    "my_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # For sequence classification\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Creating a trainable PEFT model object from a PEFT function\n",
    "\n",
    "lora_peft_model = get_peft_model(my_model, my_config)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b005892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,076 || all params: 142,048,520 || trainable%: 0.0022\n"
     ]
    }
   ],
   "source": [
    "# Confirmation on Lora is working so only some parameters can be fine-tuned\n",
    "\n",
    "lora_peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4058002a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DebertaV2ForSequenceClassification(\n",
       "      (deberta): DebertaV2Model(\n",
       "        (embeddings): DebertaV2Embeddings(\n",
       "          (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): DebertaV2Encoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x DebertaV2Layer(\n",
       "              (attention): DebertaV2Attention(\n",
       "                (self): DisentangledSelfAttention(\n",
       "                  (query_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): DebertaV2SelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): DebertaV2Intermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DebertaV2Output(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (rel_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (pooler): ContextPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=4, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observing the model object\n",
    "\n",
    "lora_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c1259bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the original peft model\n",
    "\n",
    "#lora_peft_model.save_pretrained(\"./tmp/original_lora_peft_8142133_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a847c4d1-99d7-4b58-85fe-95507269ecb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 1. Load PEFT config to find base model name\\npeft_model_path = \"./tmp/original_lora_peft_8142133_model\"  \\npeft_config = PeftConfig.from_pretrained(peft_model_path)\\n\\n# 2. Load base model with correct label count\\nmy_model = DebertaV2ForSequenceClassification.from_pretrained(\\n    \\'microsoft/deberta-v3-small\\', \\n    num_labels=4,\\n    ignore_mismatched_sizes=True,\\n    use_safetensors=True \\n    )\\n\\n# 3. Load LoRA adapter on top of base model\\noriginal_lora_peft = PeftModel.from_pretrained(my_model, peft_model_path)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Practicing correct LoRA peft load\n",
    "'''\n",
    "# 1. Load PEFT config to find base model name\n",
    "peft_model_path = \"./tmp/original_lora_peft_8142133_model\"  \n",
    "peft_config = PeftConfig.from_pretrained(peft_model_path)\n",
    "\n",
    "# 2. Load base model with correct label count\n",
    "my_model = DebertaV2ForSequenceClassification.from_pretrained(\n",
    "    'microsoft/deberta-v3-small', \n",
    "    num_labels=4,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    use_safetensors=True \n",
    "    )\n",
    "\n",
    "# 3. Load LoRA adapter on top of base model\n",
    "original_lora_peft = PeftModel.from_pretrained(my_model, peft_model_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6803c040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DebertaV2ForSequenceClassification(\n",
       "      (deberta): DebertaV2Model(\n",
       "        (embeddings): DebertaV2Embeddings(\n",
       "          (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): DebertaV2Encoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x DebertaV2Layer(\n",
       "              (attention): DebertaV2Attention(\n",
       "                (self): DisentangledSelfAttention(\n",
       "                  (query_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): DebertaV2SelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): DebertaV2Intermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DebertaV2Output(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (rel_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (pooler): ContextPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=4, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observing the retrieved model object\n",
    "\n",
    "original_lora_peft = lora_peft_model\n",
    "\n",
    "original_lora_peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54876f6",
   "metadata": {},
   "source": [
    "I am selecting the tweet_eval dataset which is a collection of tweet-based benchmark tasks designed for evaluating text classification models on social media content. It includes several sub-tasks such as emotion classification, hate speech detection, irony, stance detection, and more—each with its own labeled subset. Tweets are short, informal, and often noisy, making the dataset ideal for developing and testing models in real-world, low-resource language scenarios. The dataset is widely used for benchmarking due to its diversity and compact size, with some sub-tasks (like the \"emotion\" subset) containing fewer than 4,000 training examples—making it especially suitable for training with limited computing resource because using the dataset emotion the traning crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8b69c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfaefe9539942b4ac52fa209a007482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dslab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dslab\\.cache\\huggingface\\hub\\datasets--tweet_eval. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3257\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1421\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 374\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Now select and load a dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "#my_dataset = load_dataset(\"emotion\")\n",
    "my_dataset = load_dataset(\"tweet_eval\", \"emotion\")\n",
    "\n",
    "dataset_splits = ['train', 'validation', 'test']\n",
    "\n",
    "print(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57a316cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3257\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review overall train dataset\n",
    "\n",
    "my_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0d756d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 1421\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review overall test dataset\n",
    "\n",
    "my_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19ead520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"“Worry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\",\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the first example from the train dataset\n",
    "\n",
    "my_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0f8fb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '#Deppression is real. Partners w/ #depressed people truly dont understand the depth in which they affect us. Add in #anxiety &amp;makes it worse',\n",
       " 'label': 3}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the first example from the validation dataset\n",
    "\n",
    "my_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dbb00f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80745402b555454c833109b1d518b764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dslab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dslab\\.cache\\huggingface\\hub\\models--microsoft--deberta-v2-xlarge. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5a99a94420430c95566948ac416850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c2234709ae4d2bac37837b7d30c674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/633 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading an appropriate selected tokenizer\n",
    "\n",
    "my_tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v2-xlarge')\n",
    "\n",
    "#my_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e701217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Improved tokenizer version\n",
    "\n",
    "my_tokenized_dataset = {}\n",
    "\n",
    "for split in dataset_splits:\n",
    "    my_tokenized_dataset[split] = my_dataset[split].map(\n",
    "        #lambda x: my_tokenizer(x[\"text\"], truncation=True, padding=\"max_length\"), \n",
    "        lambda x: my_tokenizer(x[\"text\"], truncation=True, padding=True, return_tensors=\"pt\"), \n",
    "        batched=True\n",
    "    )\n",
    "\n",
    "# Inspect the available columns in the dataset\n",
    "print(my_tokenized_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12b17e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenized_dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5292b7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenized_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e9b2867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"“Worry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\", 'label': 2, 'input_ids': [1, 68, 43422, 41870, 13, 10, 184, 1574, 21, 10, 453, 17, 111, 252, 30, 25, 4, 15282, 15583, 4, 1539, 76839, 1539, 71038, 1539, 118308, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caabe4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '#Deppression is real. Partners w/ #depressed people truly dont understand the depth in which they affect us. Add in #anxiety &amp;makes it worse', 'label': 3, 'input_ids': [1, 1539, 99185, 56743, 13, 340, 4, 8583, 2564, 96, 1539, 2539, 30606, 98, 1276, 5826, 513, 5, 3291, 11, 59, 49, 2271, 120, 4, 1962, 11, 1539, 63270, 169, 10087, 93, 54082, 22, 2416, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenized_dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242356b",
   "metadata": {},
   "source": [
    "## Performing the baseline evaluation of the pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195a81a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db51ab47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1421\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "my_tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "#my_tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "#tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(my_tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bde5d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure the testing dataset is ready for evaluation\n",
    "\n",
    "my_testing_tokenized_dataset = my_tokenized_dataset[\"test\"].map(batched=True)\n",
    "\n",
    "my_testing_tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14f5860f-70c7-4623-b82b-0248158a5ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete cache before running to allow enough memory for it\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "548baa9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef4a79a4b86422f9b41591cf641797f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Evaluation: {'accuracy': 0.25193525686136525}\n"
     ]
    }
   ],
   "source": [
    "# Test initial accuracy of the pretrained Model on the selected dataset\n",
    "\n",
    "baseline_results = model_evaluating(original_lora_peft, my_testing_tokenized_dataset)\n",
    "print(\"Base Model Evaluation:\", baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e038e50",
   "metadata": {},
   "source": [
    "## Apply PEFT to fine-tune the model efficiency via training it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d49d63",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6970e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete cache before training to allow enough memory for it\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3257' max='3257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3257/3257 05:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.300300</td>\n",
       "      <td>1.330778</td>\n",
       "      <td>0.392681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3257, training_loss=1.2846859026291928, metrics={'train_runtime': 352.5412, 'train_samples_per_second': 9.239, 'train_steps_per_second': 9.239, 'total_flos': 57016668941232.0, 'train_loss': 1.2846859026291928, 'epoch': 1.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Run a training loop\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_lora_peft,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./tmp/patent_class\",\n",
    "        # Set the learning rate\n",
    "        learning_rate=2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        #evaluation_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        # Set the learning rate\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=my_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=my_tokenized_dataset[\"test\"],\n",
    "    #tokenizer=my_tokenizer,\n",
    "    processing_class=my_tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=my_tokenizer),\n",
    "    #label_names=[\"label\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a137f10-e0ad-408b-9476-0d610086b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dir = \"./tmp/finetuned_model816\"\n",
    "\n",
    "# Save model (writes config.json, pytorch_model.bin, etc.)\n",
    "\n",
    "# First merge LoRA weights with base model\n",
    "#ft_model = original_lora_peft.merge_and_unload()\n",
    "#ft_model.save_pretrained(save_dir)\n",
    "\n",
    "# Save tokenizer into the SAME folder (adds tokenizer files alongside)\n",
    "#my_tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "#final_model = original_lora_peft.merge_and_unload()\n",
    "#final_model.save_pretrained(\"./tmp/finetuned_816_model\")\n",
    "#my_tokenizer.save_pretrained(\"./tmp/finetuned_816_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e64a705d-c23a-4101-9fe4-57c2a77f2612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify immediately after saving\n",
    "\n",
    "#print(os.listdir(save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a65f6b5-149f-4b0d-afb7-13aa2c7b0a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model at: ./saved_models/finetuned_8282025_model_v4\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "version = \"v4\"  \n",
    "save_dir = f\"./saved_models/finetuned_8282025_model_{version}\"\n",
    "\n",
    "#final_model = original_lora_peft.merge_and_unload()\n",
    "original_lora_peft.save_pretrained(save_dir)\n",
    "my_tokenizer.save_pretrained(save_dir)\n",
    "original_lora_peft.config.save_pretrained(save_dir)\n",
    "\n",
    "print(\"Saved model at:\", save_dir)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86056b97-6e9d-4537-bfdb-4642eb246a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model at: ./saved_models/finetuned_8282025_model_v5\n"
     ]
    }
   ],
   "source": [
    "version = \"v5\"  \n",
    "save_dir = f\"./saved_models/finetuned_8282025_model_{version}\"\n",
    "\n",
    "# Save PEFT adapters\n",
    "original_lora_peft.save_pretrained(save_dir)\n",
    "\n",
    "# Save full model state including the classification head\n",
    "torch.save(original_lora_peft.state_dict(), f\"{save_dir}/full_model_state.bin\")\n",
    "\n",
    "# Save config and tokenizer\n",
    "original_lora_peft.config.save_pretrained(save_dir)\n",
    "my_tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"Saved model at:\", save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92f54187-0127-4ddb-a8d8-2364ce162011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adapter_config.json', 'adapter_model.safetensors', 'added_tokens.json', 'config.json', 'full_model_state.bin', 'README.md', 'special_tokens_map.json', 'spm.model', 'tokenizer_config.json']\n"
     ]
    }
   ],
   "source": [
    "# Verify immediately after saving\n",
    "\n",
    "print(os.listdir(save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cad73f-b697-4ef0-ac00-b797dd64244b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030b69f-1f19-4ccb-ae0e-1852d88e0503",
   "metadata": {},
   "source": [
    "#### Now need to perform the model and tokenizer retrieval and peform an evaluation to validate that it keeps performing better than the pretrained original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b04416-217c-4477-a1a1-9bcca0aa01cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
